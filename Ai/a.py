import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
from urllib.parse import urljoin, urlparse

def check_requirements():
    """Kiá»ƒm tra cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t"""
    try:
        import requests
        import bs4
        import pandas as pd
        print("âœ… Táº¥t cáº£ thÆ° viá»‡n Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t")
        return True
    except ImportError as e:
        print(f"âŒ Thiáº¿u thÆ° viá»‡n: {e}")
        print("Vui lÃ²ng cháº¡y: pip install requests beautifulsoup4 pandas lxml")
        return False

def explore_cooky_structure():
    """KhÃ¡m phÃ¡ cáº¥u trÃºc website cooky.vn"""
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # Thá»­ cÃ¡c URL khÃ¡c nhau
    test_urls = [
        "https://www.cooky.vn/",
        "https://www.cooky.vn/dish",
        "https://www.cooky.vn/dishes",
        "https://www.cooky.vn/recipe",
        "https://www.cooky.vn/recipes",
        "https://cooky.vn/",
        "https://cooky.vn/dish",
        "https://cooky.vn/recipes"
    ]
    
    print("ğŸ” Äang khÃ¡m phÃ¡ cáº¥u trÃºc website...")
    
    for url in test_urls:
        try:
            print(f"\nğŸŒ Kiá»ƒm tra: {url}")
            response = requests.get(url, headers=headers, timeout=10)
            print(f"   Status: {response.status_code}")
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # TÃ¬m cÃ¡c link mÃ³n Äƒn
                dish_links = soup.find_all('a', href=True)
                recipe_links = [link for link in dish_links if 'recipe' in link.get('href', '').lower() or 'dish' in link.get('href', '').lower()]
                
                if recipe_links:
                    print(f"   âœ… TÃ¬m tháº¥y {len(recipe_links)} link mÃ³n Äƒn")
                    print(f"   ğŸ“‹ VÃ­ dá»¥: {recipe_links[0].get('href')}")
                    return url, soup
                else:
                    # TÃ¬m pagination hoáº·c danh sÃ¡ch mÃ³n Äƒn
                    pagination = soup.find_all(['a', 'div'], class_=['page', 'pagination', 'dish', 'recipe'])
                    if pagination:
                        print(f"   ğŸ”— TÃ¬m tháº¥y {len(pagination)} element liÃªn quan")
                        
        except Exception as e:
            print(f"   âŒ Lá»—i: {e}")
    
    return None, None

def find_recipe_structure(base_url, soup):
    """TÃ¬m cáº¥u trÃºc cá»§a trang recipe"""
    
    print(f"\nğŸ” PhÃ¢n tÃ­ch cáº¥u trÃºc cá»§a {base_url}")
    
    # TÃ¬m cÃ¡c selector cÃ³ thá»ƒ chá»©a danh sÃ¡ch mÃ³n Äƒn
    possible_selectors = [
        '.dish-card',
        '.recipe-card', 
        '.food-card',
        '.item-card',
        '.card',
        '[class*="dish"]',
        '[class*="recipe"]',
        '[class*="food"]'
    ]
    
    for selector in possible_selectors:
        elements = soup.select(selector)
        if elements:
            print(f"   âœ… TÃ¬m tháº¥y {len(elements)} element vá»›i selector: {selector}")
            
            # TÃ¬m link trong element Ä‘áº§u tiÃªn
            first_element = elements[0]
            links = first_element.find_all('a', href=True)
            if links:
                href = links[0].get('href')
                print(f"   ğŸ”— Link máº«u: {href}")
                return selector, href
    
    return None, None

def crawl_with_discovered_structure():
    """Crawl vá»›i cáº¥u trÃºc Ä‘Æ°á»£c phÃ¡t hiá»‡n"""
    
    if not check_requirements():
        return
    
    # KhÃ¡m phÃ¡ cáº¥u trÃºc
    base_url, soup = explore_cooky_structure()
    
    if not base_url:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y cáº¥u trÃºc phÃ¹ há»£p cá»§a cooky.vn")
        print("ğŸ”„ Thá»­ crawl vá»›i cÃ¡c website khÃ¡c:")
        alternative_crawl()
        return
    
    print(f"\nğŸ¯ Sá»­ dá»¥ng base URL: {base_url}")
    
    # TÃ¬m cáº¥u trÃºc selector
    selector, sample_href = find_recipe_structure(base_url, soup)
    
    if not selector:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y cáº¥u trÃºc mÃ³n Äƒn phÃ¹ há»£p")
        return
    
    # Crawl dá»¯ liá»‡u
    crawl_cooky_dishes_new(base_url, selector)

def alternative_crawl():
    """Crawl tá»« cÃ¡c website khÃ¡c"""
    
    print("\nğŸ”„ Thá»­ crawl tá»« cÃ¡c website khÃ¡c:")
    
    # Thá»­ website khÃ¡c
    alternative_sites = [
        {
            'name': 'MÃ³n Ngon Má»—i NgÃ y',
            'url': 'https://www.monngonmoingay.com/',
            'selector': '.recipe-item'
        },
        {
            'name': 'Foody',
            'url': 'https://www.foody.vn/',
            'selector': '.recipe-card'
        }
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    for site in alternative_sites:
        try:
            print(f"\nğŸŒ Thá»­ {site['name']}: {site['url']}")
            response = requests.get(site['url'], headers=headers, timeout=10)
            
            if response.status_code == 200:
                print(f"   âœ… Káº¿t ná»‘i thÃ nh cÃ´ng!")
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # TÃ¬m cÃ¡c link mÃ³n Äƒn
                links = soup.find_all('a', href=True)
                recipe_links = [link for link in links if any(keyword in link.get('href', '').lower() for keyword in ['recipe', 'dish', 'mon-an', 'cong-thuc'])]
                
                if recipe_links:
                    print(f"   ğŸ“‹ TÃ¬m tháº¥y {len(recipe_links)} link mÃ³n Äƒn")
                    break
                    
        except Exception as e:
            print(f"   âŒ Lá»—i: {e}")

def crawl_cooky_dishes_new(base_url, selector):
    """Crawl vá»›i cáº¥u trÃºc má»›i Ä‘Æ°á»£c phÃ¡t hiá»‡n"""
    
    dishes = []
    ingredients_set = set()
    dish_ingredient_map = []
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        # Crawl trang chá»§
        response = requests.get(base_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # TÃ¬m cÃ¡c mÃ³n Äƒn
        dish_elements = soup.select(selector)[:800]  # Giá»›i háº¡n 20 mÃ³n Ä‘áº§u tiÃªn
        
        print(f"ğŸ“‹ TÃ¬m tháº¥y {len(dish_elements)} mÃ³n Äƒn")
        
        for i, element in enumerate(dish_elements, 1):
            try:
                # TÃ¬m tÃªn mÃ³n Äƒn
                dish_name = element.get_text(strip=True)
                if not dish_name:
                    continue
                
                # TÃ¬m link chi tiáº¿t
                link = element.find('a', href=True)
                if link:
                    href = link.get('href')
                    full_url = urljoin(base_url, href)
                else:
                    full_url = base_url
                
                print(f"  {i}. {dish_name[:50]}...")
                
                # ThÃªm vÃ o danh sÃ¡ch
                dishes.append({
                    "dish_id": i,
                    "name": dish_name,
                    "url_method": full_url
                })
                
                time.sleep(0.5)
                
            except Exception as e:
                print(f"    âŒ Lá»—i: {e}")
                continue
        
        # LÆ°u dá»¯ liá»‡u
        if dishes:
            save_simple_data(dishes)
        else:
            print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u nÃ o Ä‘Æ°á»£c crawl")
            
    except Exception as e:
        print(f"âŒ Lá»—i crawl: {e}")

def save_simple_data(dishes):
    """LÆ°u dá»¯ liá»‡u Ä‘Æ¡n giáº£n"""
    
    output_dir = "crawled_data"
    os.makedirs(output_dir, exist_ok=True)
    
    dish_df = pd.DataFrame(dishes)
    dish_df.to_csv(f"{output_dir}/dishes.csv", index=False, encoding='utf-8')
    
    print(f"\nâœ… ÄÃ£ lÆ°u {len(dishes)} mÃ³n Äƒn vÃ o {output_dir}/dishes.csv")

if __name__ == "__main__":
    print("ğŸš€ Báº¯t Ä‘áº§u khÃ¡m phÃ¡ vÃ  crawl dá»¯ liá»‡u...")
    crawl_with_discovered_structure()